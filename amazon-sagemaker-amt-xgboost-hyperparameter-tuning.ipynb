{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning the Iris Dataset With XGBoost on Amazon SageMaker AMT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "This example demonstrates how to use Amazon SageMaker to perform hyperparameter tuning on an XGBoost model for the Iris dataset.\n",
    "\n",
    "Hyperparameter tuning is the process of finding the optimal set of hyperparameters for a machine learning model. Hyperparameters are model parameters that are set before the training process begins, such as the learning rate and the maximum depth of the trees.\n",
    "\n",
    "This notebook covers the configuration and execution of a hyperparameter tuning job using Amazon SageMaker. The Iris dataset is prepared and split into train, validation, and test sets, which are then uploaded to Amazon S3. The hyperparameter tuning job and training job configurations are defined, and the tuning job is launched.\n",
    "\n",
    "___\n",
    "\n",
    "### 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Set up SageMaker session and S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = boto3.Session().region_name\n",
    "smclient = boto3.Session().client('sagemaker')\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "prefix = 'DEMO-iris-hpo-xgboost'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Load and prepare the Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "data = pd.DataFrame(data=np.c_[iris['data'], iris['target']], columns=iris['feature_names'] + ['target'])\n",
    "\n",
    "# Display the first few rows to verify data loading\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Split the data into training, validation, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "validation_data, test_data = train_test_split(test_data, test_size=0.5, random_state=42)\n",
    "\n",
    "# Verify the sizes of the splits\n",
    "print(f\"Training data size: {train_data.shape}\")\n",
    "print(f\"Validation data size: {validation_data.shape}\")\n",
    "print(f\"Test data size: {test_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Save and upload data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv('train.csv', index=False, header=False)\n",
    "validation_data.to_csv('validation.csv', index=False, header=False)\n",
    "\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train/train.csv')).upload_file('train.csv')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'validation/validation.csv')).upload_file('validation.csv')\n",
    "\n",
    "print(f\"Data uploaded to S3 bucket: {bucket}, prefix: {prefix}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Configure hyperparameter tuning settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_job_config = {\n",
    "    \"ParameterRanges\": {\n",
    "      \"CategoricalParameterRanges\": [],\n",
    "      \"ContinuousParameterRanges\": [\n",
    "        {\n",
    "          \"MaxValue\": \"0.5\",\n",
    "          \"MinValue\": \"0.1\",\n",
    "          \"Name\": \"eta\"\n",
    "        }\n",
    "      ],\n",
    "      \"IntegerParameterRanges\": [\n",
    "        {\n",
    "          \"MaxValue\": \"5\",\n",
    "          \"MinValue\": \"2\",\n",
    "          \"Name\": \"max_depth\"\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    \"ResourceLimits\": {\n",
    "      \"MaxNumberOfTrainingJobs\": 4,\n",
    "      \"MaxParallelTrainingJobs\": 2\n",
    "    },\n",
    "    \"Strategy\": \"Bayesian\",\n",
    "    \"HyperParameterTuningJobObjective\": {\n",
    "      \"MetricName\": \"validation:rmse\",\n",
    "      \"Type\": \"Minimize\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Get training image URI for XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_image = sagemaker.image_uris.retrieve('xgboost', region, '1.0-1')\n",
    "print(f\"Using training image: {training_image}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Configure S3 input paths for training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_input_train = f's3://{bucket}/{prefix}/train'\n",
    "s3_input_validation = f's3://{bucket}/{prefix}/validation/'\n",
    "print(f\"Training data path: {s3_input_train}\")\n",
    "print(f\"Validation data path: {s3_input_validation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Define the training job configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_job_definition = {\n",
    "    \"AlgorithmSpecification\": {\n",
    "      \"TrainingImage\": training_image,\n",
    "      \"TrainingInputMode\": \"File\"\n",
    "    },\n",
    "    \"InputDataConfig\": [\n",
    "      {\n",
    "        \"ChannelName\": \"train\",\n",
    "        \"CompressionType\": \"None\",\n",
    "        \"ContentType\": \"csv\",\n",
    "        \"DataSource\": {\n",
    "          \"S3DataSource\": {\n",
    "            \"S3DataDistributionType\": \"FullyReplicated\",\n",
    "            \"S3DataType\": \"S3Prefix\",\n",
    "            \"S3Uri\": s3_input_train\n",
    "          }\n",
    "        }\n",
    "      },\n",
    "      {\n",
    "        \"ChannelName\": \"validation\",\n",
    "        \"CompressionType\": \"None\",\n",
    "        \"ContentType\": \"csv\",\n",
    "        \"DataSource\": {\n",
    "          \"S3DataSource\": {\n",
    "            \"S3DataDistributionType\": \"FullyReplicated\",\n",
    "            \"S3DataType\": \"S3Prefix\",\n",
    "            \"S3Uri\": s3_input_validation\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    ],\n",
    "    \"OutputDataConfig\": {\n",
    "      \"S3OutputPath\": f\"s3://{bucket}/{prefix}/output\"\n",
    "    },\n",
    "    \"ResourceConfig\": {\n",
    "      \"InstanceCount\": 1,\n",
    "      \"InstanceType\": \"ml.m5.large\",\n",
    "      \"VolumeSizeInGB\": 5\n",
    "    },\n",
    "    \"RoleArn\": role,\n",
    "    \"StaticHyperParameters\": {\n",
    "      \"num_round\": \"50\",\n",
    "      \"objective\": \"reg:squarederror\",\n",
    "      \"verbosity\": \"2\"\n",
    "    },\n",
    "    \"StoppingCondition\": {\n",
    "      \"MaxRuntimeInSeconds\": 1800\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Launch the hyperparameter tuning job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_job_name = \"IrisHPO\"\n",
    "smclient.create_hyper_parameter_tuning_job(\n",
    "    HyperParameterTuningJobName=tuning_job_name,\n",
    "    HyperParameterTuningJobConfig=tuning_job_config,\n",
    "    TrainingJobDefinition=training_job_definition\n",
    ")\n",
    "\n",
    "print(f\"Launched hyperparameter tuning job: {tuning_job_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Monitoring Hyperparameter Tuning Jobs in SageMaker\n",
    "\n",
    "In the SageMaker console, you can monitor the status of the training jobs created by the hyperparameter tuning job.\n",
    "\n",
    "1. In the left navigation pane, under **Training**, click **Hyperparameter tuning jobs**, and click on the hyperparameter tuning job you launched.\n",
    "2. In the **Training jobs** section, you can view a list of individual training jobs along with their current statuses. This section provides real-time updates so you can see which jobs have been completed, which are still running, and if any have encountered errors.\n",
    "3. Click **Best training job** to view the details of the best training job and review the configurations and results for this model.\n",
    "\n",
    "Throughout the tuning process, SageMaker uses the objective metric (in this case, the validation RMSE) from each training job to determine the best-performing model. While the tuning job runs, SageMaker continuously updates which job has achieved the best objective metric. When the tuning job finishes, SageMaker highlights the training job that returned the best objective metric.\n",
    "\n",
    "After identifying the best training job, you can deploy it to a SageMaker endpoint for inference:\n",
    "1. Choose **Create model** to deploy the best training job as a model on SageMaker.\n",
    "2. Adjust the model endpoint settings as needed and proceed to launch the deployment.\n",
    "\n",
    "Deploying the model creates a SageMaker endpoint, which allows you to send data for predictions and test the model’s performance in a production-like environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Clean up\n",
    "\n",
    "To avoid incurring unnecessary charges, use the AWS Management Console to delete the resources that you created for it.\n",
    "1. Open the SageMaker console and delete the notebook instance. Stop the instance before deleting it.\n",
    "2. Open the Amazon S3 console and delete the bucket that you created to store model artifacts and the training dataset.\n",
    "3. Open the Amazon CloudWatch console and delete all of the log groups that have names starting with /aws/sagemaker/.\n",
    "\n",
    "___"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
